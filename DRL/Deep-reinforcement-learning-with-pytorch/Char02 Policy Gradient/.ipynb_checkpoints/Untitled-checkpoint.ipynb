{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "gamma = 0.99\n",
    "# env.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state) #returns the probability of an action with the given state (eg. output of the NN)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 10 # what is the running reward?\n",
    "policy = Policy()\n",
    "# optimizer updates the weights of the NN\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.from_numpy(state).float().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = policy(state) #returns the probability of an action with the given state (eg. output of the NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Categorical(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.saved_log_probs.append(m.log_prob(action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = select_action(state)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n",
      "finish episode\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a66395822e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 10000 steps per episode, when the episode is done, done = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# select action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a320040d9e61>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#returns the probability of an action with the given state (eg. output of the NN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-704f69e1d505>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maction_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):  # 10000 steps per episode, when the episode is done, done = True\n",
    "        action = select_action(state) # select action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "             break\n",
    "\n",
    "    running_reward = running_reward * 0.99 + t * 0.01\n",
    "    finish_episode()\n",
    "    print(\"finish episode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
